  482 training samples
   31 validation samples
  177 test samples
======== Epoch 1 / 3 ========
Training...
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Current Loss 0.7131531834602356
Current Loss 0.6981160640716553
Current Loss 0.6869893074035645
Current Loss 0.7057799696922302
Current Loss 0.6825907826423645
Current Loss 0.6782877445220947
Current Loss 0.7028681635856628
Current Loss 0.6988762617111206
Current Loss 0.6861779093742371
Current Loss 0.6871650218963623
Current Loss 0.6603778600692749
Current Loss 0.6840072870254517
Current Loss 0.678065299987793
Current Loss 0.6981240510940552
Current Loss 0.660279393196106
Current Loss 0.6664873361587524
Average training loss: 0.69
Training epoch took: 0:02:43
Running Validation...
  Accuracy: 0.65
  Validation Loss: 0.66
  Validation took: 0:00:03
======== Epoch 2 / 3 ========
Training...
Current Loss 0.647614598274231
Current Loss 0.6411616206169128
Current Loss 0.6438814997673035
Current Loss 0.5983243584632874
Current Loss 0.6311898231506348
Current Loss 0.5953640341758728
Current Loss 0.6347121596336365
Current Loss 0.6244904398918152
Current Loss 0.6197676658630371
Current Loss 0.6009615659713745
Current Loss 0.5799673199653625
Current Loss 0.6069414615631104
Current Loss 0.6072373390197754
Current Loss 0.6069211959838867
Current Loss 0.5845028758049011
Current Loss 0.8112033605575562
Average training loss: 0.63
Training epoch took: 0:02:35
Running Validation...
  Accuracy: 0.58
  Validation Loss: 0.65
  Validation took: 0:00:03
======== Epoch 3 / 3 ========
Training...
Current Loss 0.56378173828125
Current Loss 0.5717055797576904
Current Loss 0.49809199571609497
Current Loss 0.5126979351043701
Current Loss 0.47942858934402466
Current Loss 0.542502760887146
Current Loss 0.5307338833808899
Current Loss 0.5252919793128967
Current Loss 0.5407253503799438
Current Loss 0.6580060720443726
Current Loss 0.42785486578941345
Current Loss 0.5203891396522522
Current Loss 0.5495417714118958
Current Loss 0.48247429728507996
Current Loss 0.5323821306228638
Current Loss 0.5587725043296814
Average training loss: 0.53
Training epoch took: 0:02:35
Running Validation...
  Accuracy: 0.61
  Validation Loss: 0.62
  Validation took: 0:00:03
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.5617977528089888
Recall:  0.625
Fscore:  0.591715976331361
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.66      0.60      0.63        97
           1       0.56      0.62      0.59        80
    accuracy                           0.61       177
   macro avg       0.61      0.61      0.61       177
weighted avg       0.62      0.61      0.61       177
>>> exit()
>>> exit()
>>> exit()