  482 training samples
   31 validation samples
  177 test samples
======== Epoch 1 / 3 ========
Training...
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Current Loss 0.6870110034942627
Current Loss 0.6830862760543823
Current Loss 0.6840432286262512
Current Loss 0.7229400277137756
Current Loss 0.6959186792373657
Current Loss 0.6760357022285461
Current Loss 0.7057129740715027
Current Loss 0.6734390258789062
Current Loss 0.689817488193512
Current Loss 0.6905938386917114
Current Loss 0.6811327934265137
Current Loss 0.6701841354370117
Current Loss 0.695004940032959
Current Loss 0.6810545921325684
Current Loss 0.7021535634994507
Current Loss 0.5443836450576782
Average training loss: 0.68
Training epoch took: 0:03:14
Running Validation...
  Accuracy: 0.42
  Validation Loss: 0.69
  Validation took: 0:00:03
======== Epoch 2 / 3 ========
Training...
Current Loss 0.6646739840507507
Current Loss 0.6511028409004211
Current Loss 0.6561271548271179
Current Loss 0.6636258959770203
Current Loss 0.6250534057617188
Current Loss 0.6200449466705322
Current Loss 0.6484658718109131
Current Loss 0.6415839791297913
Current Loss 0.6255494356155396
Current Loss 0.610954761505127
Current Loss 0.6278310418128967
Current Loss 0.6438526511192322
Current Loss 0.5976484417915344
Current Loss 0.6509889364242554
Current Loss 0.6746116280555725
Current Loss 0.5963776707649231
Average training loss: 0.64
Training epoch took: 0:02:57
Running Validation...
  Accuracy: 0.52
  Validation Loss: 0.69
  Validation took: 0:00:03
======== Epoch 3 / 3 ========
Training...
Current Loss 0.5420721769332886
Current Loss 0.5075033903121948
Current Loss 0.5467643737792969
Current Loss 0.5297704935073853
Current Loss 0.6114364266395569
Current Loss 0.617618978023529
Current Loss 0.55035799741745
Current Loss 0.6059636473655701
Current Loss 0.5405592918395996
Current Loss 0.6092205047607422
Current Loss 0.5552301406860352
Current Loss 0.5707708597183228
Current Loss 0.6139828562736511
Current Loss 0.6114011406898499
Current Loss 0.5666221380233765
Current Loss 0.6599182486534119
Average training loss: 0.58
Training epoch took: 0:02:55
Running Validation...
  Accuracy: 0.55
  Validation Loss: 0.67
  Validation took: 0:00:03
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.6753246753246753
Recall:  0.5842696629213483
Fscore:  0.6265060240963856
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.63      0.72      0.67        88
           1       0.68      0.58      0.63        89
    accuracy                           0.65       177
   macro avg       0.65      0.65      0.65       177
weighted avg       0.65      0.65      0.65       177
>>> exit()
>>> exit()
>>> exit()