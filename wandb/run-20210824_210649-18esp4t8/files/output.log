  482 training samples
   31 validation samples
  177 test samples
======== Epoch 1 / 3 ========
Training...
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Current Loss 0.7083524465560913
Current Loss 0.7207635045051575
Current Loss 0.6907036900520325
Current Loss 0.723590075969696
Current Loss 0.718354344367981
Current Loss 0.6897031664848328
Current Loss 0.6936584711074829
Current Loss 0.7029051184654236
Current Loss 0.6840293407440186
Current Loss 0.6963977217674255
Current Loss 0.6995469331741333
Current Loss 0.693926990032196
Current Loss 0.6933624148368835
Current Loss 0.7040493488311768
Current Loss 0.7016576528549194
Current Loss 0.7049909234046936
Average training loss: 0.70
Training epoch took: 0:02:58
Running Validation...
  Accuracy: 0.52
  Validation Loss: 0.68
  Validation took: 0:00:03
======== Epoch 2 / 3 ========
Training...
Current Loss 0.6772175431251526
Current Loss 0.6709068417549133
Current Loss 0.6849045157432556
Current Loss 0.6657583117485046
Current Loss 0.7017509341239929
Current Loss 0.6641811728477478
Current Loss 0.692798376083374
Current Loss 0.6713594794273376
Current Loss 0.6705295443534851
Current Loss 0.6521010994911194
Current Loss 0.6609524488449097
Current Loss 0.6804049015045166
Current Loss 0.6379762291908264
Current Loss 0.6460050344467163
Current Loss 0.6579678654670715
Current Loss 0.6089637875556946
Average training loss: 0.67
Training epoch took: 0:02:38
Running Validation...
  Accuracy: 0.58
  Validation Loss: 0.66
  Validation took: 0:00:03
======== Epoch 3 / 3 ========
Training...
Current Loss 0.619243860244751
Current Loss 0.6478059887886047
Current Loss 0.6642974615097046
Current Loss 0.6318676471710205
Current Loss 0.6424879431724548
Current Loss 0.6605251431465149
Current Loss 0.7271833419799805
Current Loss 0.6112015843391418
Current Loss 0.6550830602645874
Current Loss 0.6520102024078369
Current Loss 0.6105016469955444
Current Loss 0.6246517300605774
Current Loss 0.6775236129760742
Current Loss 0.6127327680587769
Current Loss 0.6417868137359619
Current Loss 0.5154100060462952
Average training loss: 0.64
Training epoch took: 0:02:30
Running Validation...
  Accuracy: 0.58
  Validation Loss: 0.65
  Validation took: 0:00:03
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.5166666666666667
Recall:  0.6966292134831461
Fscore:  0.5933014354066987
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.53      0.34      0.41        88
           1       0.52      0.70      0.59        89
    accuracy                           0.52       177
   macro avg       0.52      0.52      0.50       177
weighted avg       0.52      0.52      0.50       177