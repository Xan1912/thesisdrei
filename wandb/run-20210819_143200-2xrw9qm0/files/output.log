  482 training samples
   41 validation samples
  167 test samples
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
======== Epoch 1 / 4 ========
Training...
Current Loss 0.7078848481178284
Current Loss 0.7007679343223572
Current Loss 0.6809611320495605
Current Loss 0.7063043713569641
Current Loss 0.6990292072296143
Current Loss 0.7054474353790283
Current Loss 0.6915181279182434
Current Loss 0.7006539106369019
Current Loss 0.7004005908966064
Current Loss 0.69290691614151
Current Loss 0.6933507919311523
Current Loss 0.6834946274757385
Current Loss 0.6921250224113464
Current Loss 0.7012560963630676
Current Loss 0.6779427528381348
Current Loss 0.6778016090393066
Average training loss: 0.69
Training epoch took: 0:03:04
Running Validation...
  Accuracy: 0.58
  Validation Loss: 0.69
  Validation took: 0:00:04
======== Epoch 2 / 4 ========
Training...
Current Loss 0.6915262937545776
Current Loss 0.689356803894043
Current Loss 0.6866574287414551
Current Loss 0.6893793940544128
Current Loss 0.6849596500396729
Current Loss 0.6940359473228455
Current Loss 0.6893331408500671
Current Loss 0.6803275942802429
Current Loss 0.6791887879371643
Current Loss 0.6944447755813599
Current Loss 0.6975690722465515
Current Loss 0.6932058334350586
Current Loss 0.6847677826881409
Current Loss 0.676331639289856
Current Loss 0.6947662830352783
Current Loss 0.7331820726394653
Average training loss: 0.69
Training epoch took: 0:02:54
Running Validation...
  Accuracy: 0.63
  Validation Loss: 0.68
  Validation took: 0:00:05
======== Epoch 3 / 4 ========
Training...
Current Loss 0.6812899708747864
Current Loss 0.6714382171630859
Current Loss 0.6786003112792969
Current Loss 0.6774581670761108
Current Loss 0.6789814829826355
Current Loss 0.6691675782203674
Current Loss 0.6776289343833923
Current Loss 0.656647801399231
Current Loss 0.6803994178771973
Current Loss 0.6943881511688232
Current Loss 0.6893437504768372
Current Loss 0.6896638870239258
Current Loss 0.6777441501617432
Current Loss 0.6666790246963501
Current Loss 0.6719532608985901
Current Loss 0.6819109320640564
Average training loss: 0.68
Training epoch took: 0:03:15
Running Validation...
  Accuracy: 0.72
  Validation Loss: 0.68
  Validation took: 0:00:04
======== Epoch 4 / 4 ========
Training...
Current Loss 0.6842327117919922
Current Loss 0.6598818898200989
Current Loss 0.6846602559089661
Current Loss 0.6775546669960022
Current Loss 0.6905001401901245
Current Loss 0.6712120175361633
Current Loss 0.6748498678207397
Current Loss 0.6552329659461975
Current Loss 0.663086473941803
Current Loss 0.6831536293029785
Current Loss 0.6833524703979492
Current Loss 0.673079788684845
Current Loss 0.6679103374481201
Current Loss 0.6628434062004089
Current Loss 0.658391535282135
Current Loss 0.6867260932922363
Average training loss: 0.67
Training epoch took: 0:05:52
Running Validation...
  Accuracy: 0.70
  Validation Loss: 0.68
  Validation took: 0:00:04
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.625
Recall:  0.4430379746835443
Fscore:  0.5185185185185185
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.60      0.76      0.67        88
           1       0.62      0.44      0.52        79
    accuracy                           0.61       167
   macro avg       0.61      0.60      0.60       167
weighted avg       0.61      0.61      0.60       167
  482 training samples
   41 validation samples
  167 test samples
======== Epoch 1 / 4 ========
Training...
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Current Loss 0.6754513382911682
Current Loss 0.716713011264801
Current Loss 0.6893294453620911
Current Loss 0.693916916847229
Current Loss 0.6765214204788208
Current Loss 0.6946004629135132
Current Loss 0.6894218921661377
Current Loss 0.6794381737709045
Current Loss 0.6735755801200867
Current Loss 0.7038215398788452
Current Loss 0.6814565658569336
Current Loss 0.6726094484329224
Current Loss 0.6836408376693726
Current Loss 0.6788648366928101
Current Loss 0.6782023906707764
Current Loss 0.6736322641372681
Average training loss: 0.69
Training epoch took: 0:03:37
Running Validation...
  Accuracy: 0.51
  Validation Loss: 0.68
  Validation took: 0:00:06
======== Epoch 2 / 4 ========
Training...
Current Loss 0.6781455278396606
Current Loss 0.6762687563896179
Current Loss 0.6553853154182434
Current Loss 0.6466277241706848
Current Loss 0.6789287328720093
Current Loss 0.6230928301811218
Current Loss 0.6462436318397522
Current Loss 0.6653607487678528
Current Loss 0.6334456205368042
Current Loss 0.6438360214233398
Current Loss 0.6953355073928833
Current Loss 0.6506824493408203
Current Loss 0.6246474981307983
Current Loss 0.6656270623207092
Current Loss 0.6699973344802856
Current Loss 0.5075975060462952
Average training loss: 0.65
Training epoch took: 0:04:23
Running Validation...
  Accuracy: 0.72
  Validation Loss: 0.65
  Validation took: 0:00:07
======== Epoch 3 / 4 ========
Training...
Current Loss 0.6518126130104065
Current Loss 0.5818556547164917
Current Loss 0.5957623720169067
Current Loss 0.6224361062049866
Current Loss 0.6041091680526733
Current Loss 0.6017665863037109
Current Loss 0.5909245610237122
Current Loss 0.6172503232955933
Current Loss 0.6344031691551208
Current Loss 0.5815309286117554
Current Loss 0.5889454483985901
Current Loss 0.5954635143280029
Current Loss 0.5823508501052856
Current Loss 0.5767922401428223
Current Loss 0.6729716062545776
Current Loss 0.6799507141113281
Average training loss: 0.61
Training epoch took: 0:04:03
Running Validation...
  Accuracy: 0.55
  Validation Loss: 0.67
  Validation took: 0:00:04
======== Epoch 4 / 4 ========
Training...
Current Loss 0.569033682346344
Current Loss 0.6238166689872742
Current Loss 0.5928037762641907
Current Loss 0.5553774833679199
Current Loss 0.6054655909538269
Current Loss 0.596215009689331
Current Loss 0.6129920482635498
Current Loss 0.5195211172103882
Current Loss 0.5529893040657043
Current Loss 0.5939144492149353
Current Loss 0.5713810324668884
Current Loss 0.5382081270217896
Current Loss 0.5521406531333923
Current Loss 0.5747988820075989
Current Loss 0.5725104212760925
Current Loss 0.5456289052963257
Average training loss: 0.57
Training epoch took: 0:03:28
Running Validation...
  Accuracy: 0.51
  Validation Loss: 0.65
  Validation took: 0:00:04
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.6933333333333334
Recall:  0.6190476190476191
Fscore:  0.6540880503144655
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.65      0.72      0.69        83
           1       0.69      0.62      0.65        84
    accuracy                           0.67       167
   macro avg       0.67      0.67      0.67       167
weighted avg       0.67      0.67      0.67       167
  482 training samples
   41 validation samples
  167 test samples
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
======== Epoch 1 / 4 ========
Training...
Current Loss 0.6754513382911682
Current Loss 0.7189868688583374
Current Loss 0.689167320728302
Current Loss 0.6909334659576416
Current Loss 0.6730108261108398
Current Loss 0.7003645896911621
Current Loss 0.6952663064002991
Current Loss 0.684194028377533
Current Loss 0.6675649881362915
Current Loss 0.7050200700759888
Current Loss 0.6800798177719116
Current Loss 0.6681347489356995
Current Loss 0.6815669536590576
Current Loss 0.6725139021873474
Current Loss 0.6731638312339783
Current Loss 0.6908601522445679
Average training loss: 0.69
Training epoch took: 0:03:02
Running Validation...
  Accuracy: 0.48
  Validation Loss: 0.68
  Validation took: 0:00:04
======== Epoch 2 / 4 ========
Training...
Current Loss 0.6651102900505066
Current Loss 0.6658650636672974
Current Loss 0.634909987449646
Current Loss 0.618261992931366
Current Loss 0.677664041519165
Current Loss 0.6010867357254028
Current Loss 0.6088924407958984
Current Loss 0.6443819403648376
Current Loss 0.5939669013023376
Current Loss 0.6017577648162842
Current Loss 0.6807228326797485
Current Loss 0.636042058467865
Current Loss 0.577927827835083
Current Loss 0.6522807478904724
Current Loss 0.6493253111839294
Current Loss 0.5071461200714111
Average training loss: 0.63
Training epoch took: 0:03:05
Running Validation...
  Accuracy: 0.53
  Validation Loss: 0.66
  Validation took: 0:00:04
======== Epoch 3 / 4 ========
Training...
Current Loss 0.5893778204917908
Current Loss 0.5083382725715637
Current Loss 0.5454518795013428
Current Loss 0.5796884894371033
Current Loss 0.5375030040740967
Current Loss 0.5279113054275513
Current Loss 0.5072041749954224
Current Loss 0.5392879843711853
Current Loss 0.5943698287010193
Current Loss 0.5073582530021667
Current Loss 0.49931710958480835
Current Loss 0.533570408821106
Current Loss 0.508063554763794
Current Loss 0.49941563606262207
Current Loss 0.6578501462936401
Current Loss 0.6930586099624634
Average training loss: 0.55
Training epoch took: 0:02:50
Running Validation...
  Accuracy: 0.55
  Validation Loss: 0.68
  Validation took: 0:00:04
======== Epoch 4 / 4 ========
Training...
Current Loss 0.48186174035072327
Current Loss 0.5480563640594482
Current Loss 0.5110006928443909
Current Loss 0.4753522276878357
Current Loss 0.5304383039474487
Current Loss 0.5205303430557251
Current Loss 0.5245727896690369
Current Loss 0.41648775339126587
Current Loss 0.4684108793735504
Current Loss 0.5262422561645508
Current Loss 0.4744126796722412
Current Loss 0.41899436712265015
Current Loss 0.4455544054508209
Current Loss 0.5005916953086853
Current Loss 0.4681810140609741
Current Loss 0.39718055725097656
Average training loss: 0.48
Training epoch took: 0:02:52
Running Validation...
  Accuracy: 0.49
  Validation Loss: 0.63
  Validation took: 0:00:04
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.6666666666666666
Recall:  0.6190476190476191
Fscore:  0.6419753086419754
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.64      0.69      0.66        83
           1       0.67      0.62      0.64        84
    accuracy                           0.65       167
   macro avg       0.65      0.65      0.65       167
weighted avg       0.65      0.65      0.65       167
  482 training samples
   41 validation samples
  167 test samples
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
======== Epoch 1 / 4 ========
Training...
Current Loss 0.6754513382911682
Current Loss 0.7212631702423096
Current Loss 0.6890945434570312
Current Loss 0.6876253485679626
Current Loss 0.6719849109649658
Current Loss 0.7095727920532227
Current Loss 0.7026413679122925
Current Loss 0.6879607439041138
Current Loss 0.6631446480751038
Current Loss 0.701469898223877
Current Loss 0.6774783134460449
Current Loss 0.6650969386100769
Current Loss 0.6808594465255737
Current Loss 0.6663714051246643
Current Loss 0.6694964170455933
Current Loss 0.71842360496521
Average training loss: 0.69
Training epoch took: 0:02:45
Running Validation...
  Accuracy: 0.52
  Validation Loss: 0.69
  Validation took: 0:00:04
======== Epoch 2 / 4 ========
Training...
Current Loss 0.6543834805488586
Current Loss 0.6567279100418091
Current Loss 0.6109820604324341
Current Loss 0.5930346846580505
Current Loss 0.6809289455413818
Current Loss 0.5865522623062134
Current Loss 0.5804252624511719
Current Loss 0.6294047832489014
Current Loss 0.5513157844543457
Current Loss 0.5624347925186157
Current Loss 0.6414058208465576
Current Loss 0.6372449398040771
Current Loss 0.5549143552780151
Current Loss 0.6339293122291565
Current Loss 0.6265811324119568
Current Loss 0.625284731388092
Average training loss: 0.61
Training epoch took: 0:02:30
Running Validation...
  Accuracy: 0.51
  Validation Loss: 0.68
  Validation took: 0:00:04
======== Epoch 3 / 4 ========
Training...
Current Loss 0.5309349298477173
Current Loss 0.44218429923057556
Current Loss 0.5187889933586121
Current Loss 0.5534247159957886
Current Loss 0.4990939199924469
Current Loss 0.480615496635437
Current Loss 0.4484313130378723
Current Loss 0.46766963601112366
Current Loss 0.5447837710380554
Current Loss 0.4158582091331482
Current Loss 0.4068678617477417
Current Loss 0.4552344083786011
Current Loss 0.44996780157089233
Current Loss 0.4540878236293793
Current Loss 0.649480938911438
Current Loss 0.7736301422119141
Average training loss: 0.51
Training epoch took: 0:02:57
Running Validation...
  Accuracy: 0.57
  Validation Loss: 0.71
  Validation took: 0:00:04
======== Epoch 4 / 4 ========
Training...
Current Loss 0.4390740394592285
Current Loss 0.5035163164138794
Current Loss 0.4333690106868744
Current Loss 0.43565309047698975
Current Loss 0.4572456479072571
Current Loss 0.44301658868789673
Current Loss 0.42872685194015503
Current Loss 0.3483828604221344
Current Loss 0.3782685697078705
Current Loss 0.4702080190181732
Current Loss 0.39289894700050354
Current Loss 0.34738075733184814
Current Loss 0.3696177899837494
Current Loss 0.44406822323799133
Current Loss 0.4023330807685852
Current Loss 0.24996337294578552
Average training loss: 0.41
Training epoch took: 0:02:53
Running Validation...
  Accuracy: 0.49
  Validation Loss: 0.65
  Validation took: 0:00:04
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.6582278481012658
Recall:  0.6190476190476191
Fscore:  0.6380368098159509
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.64      0.67      0.65        83
           1       0.66      0.62      0.64        84
    accuracy                           0.65       167
   macro avg       0.65      0.65      0.65       167
weighted avg       0.65      0.65      0.65       167
  482 training samples
   41 validation samples
  167 test samples
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
======== Epoch 1 / 4 ========
Training...
Current Loss 0.6754513382911682
Current Loss 0.7234975099563599
Current Loss 0.6888951659202576
Current Loss 0.6842607259750366
Current Loss 0.6725988984107971
Current Loss 0.7197808027267456
Current Loss 0.7115732431411743
Current Loss 0.6897382140159607
Current Loss 0.6602291464805603
Current Loss 0.6949865221977234
Current Loss 0.6753330230712891
Current Loss 0.6618364453315735
Current Loss 0.679108738899231
Current Loss 0.6587120294570923
Current Loss 0.6651132106781006
Current Loss 0.7454482316970825
Average training loss: 0.69
Training epoch took: 0:02:52
Running Validation...
  Accuracy: 0.53
  Validation Loss: 0.69
  Validation took: 0:00:04
======== Epoch 2 / 4 ========
Training...
Current Loss 0.643373429775238
Current Loss 0.6413167119026184
Current Loss 0.5838911533355713
Current Loss 0.5726439356803894
Current Loss 0.6907690763473511
Current Loss 0.567106306552887
Current Loss 0.5491084456443787
Current Loss 0.6030780076980591
Current Loss 0.5235291123390198
Current Loss 0.5371084213256836
Current Loss 0.5979518294334412
Current Loss 0.6421627402305603
Current Loss 0.5223748683929443
Current Loss 0.6361109614372253
Current Loss 0.6077861189842224
Current Loss 0.593015193939209
Average training loss: 0.59
Training epoch took: 0:02:51
Running Validation...
  Accuracy: 0.48
  Validation Loss: 0.68
  Validation took: 0:00:04
======== Epoch 3 / 4 ========
Training...
Current Loss 0.5006073713302612
Current Loss 0.36845555901527405
Current Loss 0.46220314502716064
Current Loss 0.5160920023918152
Current Loss 0.45844438672065735
Current Loss 0.4095602333545685
Current Loss 0.3897254765033722
Current Loss 0.39359986782073975
Current Loss 0.48947665095329285
Current Loss 0.36836546659469604
Current Loss 0.3300321102142334
Current Loss 0.39705440402030945
Current Loss 0.3936155438423157
Current Loss 0.4013332724571228
Current Loss 0.6293285489082336
Current Loss 0.8449407815933228
Average training loss: 0.46
Training epoch took: 0:02:34
Running Validation...
  Accuracy: 0.59
  Validation Loss: 0.75
  Validation took: 0:00:05
======== Epoch 4 / 4 ========
Training...
Current Loss 0.36980682611465454
Current Loss 0.40324005484580994
Current Loss 0.35488077998161316
Current Loss 0.34421515464782715
Current Loss 0.37924081087112427
Current Loss 0.3722008168697357
Current Loss 0.36386141180992126
Current Loss 0.25332018733024597
Current Loss 0.31516534090042114
Current Loss 0.42041879892349243
Current Loss 0.3273076117038727
Current Loss 0.2777397632598877
Current Loss 0.29072070121765137
Current Loss 0.39177238941192627
Current Loss 0.3279770016670227
Current Loss 0.1827860176563263
Average training loss: 0.34
Training epoch took: 0:02:31
Running Validation...
  Accuracy: 0.48
  Validation Loss: 0.69
  Validation took: 0:00:04
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.6785714285714286
Recall:  0.6785714285714286
Fscore:  0.6785714285714286
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.67      0.67      0.67        83
           1       0.68      0.68      0.68        84
    accuracy                           0.68       167
   macro avg       0.68      0.68      0.68       167
weighted avg       0.68      0.68      0.68       167


>>> exit()