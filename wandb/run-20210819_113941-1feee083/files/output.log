  482 training samples
   41 validation samples
  167 test samples
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
[34m[1mwandb[39m[22m: Agent Starting Run: cdbbx53q with config:
[34m[1mwandb[39m[22m: 	batch_size: 32
[34m[1mwandb[39m[22m: 	epochs: 3
[34m[1mwandb[39m[22m: 	eps: 1e-08
[34m[1mwandb[39m[22m: 	lr: 2e-05
[34m[1mwandb[39m[22m: 	seed_val: 42
[34m[1mwandb[39m[22m: 	train_set_size: 0.7
======== Epoch 1 / 3 ========
Training...
Current Loss 0.666800856590271
Current Loss 0.7044927477836609
Current Loss 0.6733483672142029
Current Loss 0.6949728727340698
Current Loss 0.7224112749099731
Current Loss 0.7196906208992004
Current Loss 0.7000988721847534
Current Loss 0.6765467524528503
Current Loss 0.7262271642684937
Current Loss 0.6738569736480713
Current Loss 0.6916972398757935
Current Loss 0.7053245306015015
Current Loss 0.7075818181037903
Current Loss 0.6977745890617371
Current Loss 0.7239972352981567
Current Loss 0.7450867891311646
Average training loss: 0.70
Training epoch took: 0:02:41
Running Validation...
  Accuracy: 0.44
  Validation Loss: 0.70
  Validation took: 0:00:04
======== Epoch 2 / 3 ========
Training...
Current Loss 0.6833111047744751
Current Loss 0.692723274230957
Current Loss 0.6815989017486572
Current Loss 0.6949169039726257
Current Loss 0.6966919302940369
Current Loss 0.6924120783805847
Current Loss 0.6846517324447632
Current Loss 0.6969344615936279
Current Loss 0.6942769289016724
Current Loss 0.6794096827507019
Current Loss 0.6827612519264221
Current Loss 0.6951673030853271
Current Loss 0.6895433664321899
Current Loss 0.6969789266586304
Current Loss 0.6810687780380249
Current Loss 0.6417651176452637
Average training loss: 0.69
Training epoch took: 0:02:34
Running Validation...
  Accuracy: 0.67
  Validation Loss: 0.69
  Validation took: 0:00:04
======== Epoch 3 / 3 ========
Training...
Current Loss 0.6974913477897644
Current Loss 0.680590808391571
Current Loss 0.6914295554161072
Current Loss 0.6721615195274353
Current Loss 0.6882630586624146
Current Loss 0.7040842771530151
Current Loss 0.6791707277297974
Current Loss 0.6672453880310059
Current Loss 0.6818051934242249
Current Loss 0.6813392639160156
Current Loss 0.6765984892845154
Current Loss 0.6996669769287109
Current Loss 0.686432421207428
Current Loss 0.6838120222091675
Current Loss 0.6872912049293518
Current Loss 0.676378607749939
Average training loss: 0.68
Training epoch took: 0:02:37
Running Validation...
  Accuracy: 0.67
  Validation Loss: 0.69
  Validation took: 0:00:05