  482 training samples
   31 validation samples
  177 test samples
======== Epoch 1 / 3 ========
Training...
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Current Loss 0.7044889330863953
Current Loss 0.6900025010108948
Current Loss 0.6892930865287781
Current Loss 0.6936535835266113
Current Loss 0.6789852380752563
Current Loss 0.6996464133262634
Current Loss 0.6865906715393066
Current Loss 0.6730326414108276
Current Loss 0.6947605609893799
Current Loss 0.6677608489990234
Current Loss 0.6719524264335632
Current Loss 0.6524401903152466
Current Loss 0.6593329310417175
Current Loss 0.6575394868850708
Current Loss 0.6434664130210876
Current Loss 0.743895411491394
Average training loss: 0.68
Training epoch took: 0:02:38
Running Validation...
  Accuracy: 0.65
  Validation Loss: 0.63
  Validation took: 0:00:03
======== Epoch 2 / 3 ========
Training...
Current Loss 0.6254491209983826
Current Loss 0.5959270596504211
Current Loss 0.5935821533203125
Current Loss 0.6141551733016968
Current Loss 0.5372664332389832
Current Loss 0.632774829864502
Current Loss 0.5792878866195679
Current Loss 0.5862879157066345
Current Loss 0.5333640575408936
Current Loss 0.5940448045730591
Current Loss 0.5339227318763733
Current Loss 0.6041531562805176
Current Loss 0.5050029158592224
Current Loss 0.4588353931903839
Current Loss 0.5669790506362915
Current Loss 1.212516188621521
Average training loss: 0.61
Training epoch took: 0:02:31
Running Validation...
  Accuracy: 0.68
  Validation Loss: 0.55
  Validation took: 0:00:03
======== Epoch 3 / 3 ========
Training...
Current Loss 0.37202930450439453
Current Loss 0.4398133456707001
Current Loss 0.514359712600708
Current Loss 0.39210811257362366
Current Loss 0.47342032194137573
Current Loss 0.4672807455062866
Current Loss 0.3621954023838043
Current Loss 0.41177815198898315
Current Loss 0.4039406180381775
Current Loss 0.5111570358276367
Current Loss 0.4454916715621948
Current Loss 0.4448200762271881
Current Loss 0.3772582411766052
Current Loss 0.5484163165092468
Current Loss 0.5457257628440857
Current Loss 0.24900326132774353
Average training loss: 0.43
Training epoch took: 0:02:28
Running Validation...
  Accuracy: 0.71
  Validation Loss: 0.54
  Validation took: 0:00:03
 DONE.
******************************* Classification report: *******************************
Report via manual evaluation:
Precision:  0.5955056179775281
Recall:  0.6091954022988506
Fscore:  0.6022727272727273
Report via SKlearn:
              precision    recall  f1-score   support
           0       0.61      0.60      0.61        90
           1       0.60      0.61      0.60        87
    accuracy                           0.60       177
   macro avg       0.60      0.60      0.60       177
weighted avg       0.60      0.60      0.60       177
>>> exit(
>>> exit()
>>> exit()
>>> exit()